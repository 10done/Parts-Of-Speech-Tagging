{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken,', 'NNP'),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old,', 'JJ'),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29.', 'CD')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(json_file):\n",
    "    \"\"\"\n",
    "    Load the dataset from the given JSON file.\n",
    "    Each sentence is a list of (word, tag) tuples.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    sentences = []\n",
    "    for sentence, tags in data:\n",
    "        sentence_tokens = sentence.split()\n",
    "        sentence_with_tags = list(zip(sentence_tokens, tags))\n",
    "        sentences.append(sentence_with_tags)\n",
    "    return sentences\n",
    "sentences=load_data('penn-data.json')\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(sentences, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Split the dataset into training and testing sets.\n",
    "    \"\"\"\n",
    "    random.shuffle(sentences)\n",
    "    split_idx = int(len(sentences) * (1 - test_ratio))\n",
    "    return sentences[:split_idx], sentences[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_4tags(tag):\n",
    "    \"\"\"Collapse 36 tags into 4 categories: N, V, A, O.\"\"\"\n",
    "    if tag.startswith('N'):  # Nouns\n",
    "        return 'N'\n",
    "    elif tag.startswith('V'):  # Verbs\n",
    "        return 'V'\n",
    "    elif tag.startswith('JJ') or tag.startswith('RB'):  # Adjectives/Adverbs\n",
    "        return 'A'\n",
    "    else:  # Other\n",
    "        return 'O'\n",
    "\n",
    "def preprocess_4tag(sentences):\n",
    "    \"\"\"Convert 36-tag sentences to 4-tag sentences.\"\"\"\n",
    "    return [[(word, map_to_4tags(tag)) for (word, tag) in sentence] for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_a(train_sentences):\n",
    "    \"\"\"Train First-Order HMM with add-one smoothing.\"\"\"\n",
    "    transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "    initial_counts = defaultdict(int)\n",
    "    emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "    tag_counts = defaultdict(int)\n",
    "    all_tags = set()\n",
    "    all_words = set()\n",
    "\n",
    "    for sentence in train_sentences:\n",
    "        prev_tag = None\n",
    "        for i, (word, tag) in enumerate(sentence):\n",
    "            all_tags.add(tag)\n",
    "            all_words.add(word)\n",
    "            tag_counts[tag] += 1\n",
    "            emission_counts[tag][word] += 1\n",
    "            if i == 0:\n",
    "                initial_counts[tag] += 1\n",
    "            else:\n",
    "                transition_counts[prev_tag][tag] += 1\n",
    "            prev_tag = tag\n",
    "\n",
    "    # Add-one smoothing for initial probabilities\n",
    "    num_tags = len(all_tags)\n",
    "    initial_probs = defaultdict(lambda: -math.inf)\n",
    "    total_initial = sum(initial_counts.values()) + num_tags\n",
    "    for tag in all_tags:\n",
    "        count = initial_counts.get(tag, 0) + 1\n",
    "        initial_probs[tag] = math.log(count / total_initial)\n",
    "\n",
    "    # Add-one smoothing for transition probabilities\n",
    "    transition_probs = defaultdict(lambda: defaultdict(lambda: -math.inf))\n",
    "    for prev_tag in all_tags:\n",
    "        total = sum(transition_counts[prev_tag].values()) + num_tags\n",
    "        for curr_tag in all_tags:\n",
    "            count = transition_counts[prev_tag].get(curr_tag, 0) + 1\n",
    "            transition_probs[prev_tag][curr_tag] = math.log(count / total)\n",
    "\n",
    "    # Add-one smoothing for emission probabilities\n",
    "    num_words = len(all_words)\n",
    "    emission_probs = defaultdict(lambda: defaultdict(lambda: -math.inf))\n",
    "    for tag in all_tags:\n",
    "        total_emissions = tag_counts[tag] + num_words + 1\n",
    "        for word in all_words:\n",
    "            count = emission_counts[tag].get(word, 0) + 1\n",
    "            emission_probs[tag][word] = math.log(count / total_emissions)\n",
    "        emission_probs[tag]['UNK'] = math.log(1 / total_emissions)  # For unseen words\n",
    "\n",
    "    # Most frequent tag for unseen words\n",
    "    most_freq_tag = max(tag_counts, key=lambda k: tag_counts[k])\n",
    "    return initial_probs, transition_probs, emission_probs, most_freq_tag, all_tags, all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_a(sentence, initial_probs, transition_probs, emission_probs, most_freq_tag, all_tags, all_words):\n",
    "    \"\"\"Predict the best tag sequence using the Viterbi algorithm.\"\"\"\n",
    "    words = [word for word, _ in sentence]\n",
    "    n = len(words)\n",
    "    viterbi = [{}]\n",
    "    backpointers = [{}]\n",
    "\n",
    "    # Initialize first step\n",
    "    for tag in all_tags:\n",
    "        word = words[0]\n",
    "        if word not in all_words:\n",
    "            emit_prob = 0.0 if tag == most_freq_tag else -math.inf\n",
    "        else:\n",
    "            emit_prob = emission_probs[tag].get(word, emission_probs[tag]['UNK'])\n",
    "        viterbi[0][tag] = initial_probs[tag] + emit_prob\n",
    "        backpointers[0][tag] = None\n",
    "\n",
    "    # Recursion\n",
    "    for t in range(1, n):\n",
    "        viterbi.append({})\n",
    "        backpointers.append({})\n",
    "        word = words[t]\n",
    "        for curr_tag in all_tags:\n",
    "            max_prob = -math.inf\n",
    "            best_prev_tag = None\n",
    "            if word not in all_words:\n",
    "                emit_prob = 0.0 if curr_tag == most_freq_tag else -math.inf\n",
    "            else:\n",
    "                emit_prob = emission_probs[curr_tag].get(word, emission_probs[curr_tag]['UNK'])\n",
    "            for prev_tag in all_tags:\n",
    "                prob = viterbi[t-1].get(prev_tag, -math.inf) + transition_probs[prev_tag][curr_tag] + emit_prob\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    best_prev_tag = prev_tag\n",
    "            if best_prev_tag is not None:\n",
    "                viterbi[t][curr_tag] = max_prob\n",
    "                backpointers[t][curr_tag] = best_prev_tag\n",
    "\n",
    "    # Backtracking\n",
    "    best_path = []\n",
    "    max_final = max(viterbi[-1].values(), default=-math.inf)\n",
    "    best_tag = next((tag for tag, prob in viterbi[-1].items() if prob == max_final), None)\n",
    "    if best_tag is None:\n",
    "        return [most_freq_tag] * n\n",
    "    best_path.append(best_tag)\n",
    "    for t in reversed(range(1, n)):\n",
    "        best_tag = backpointers[t][best_tag]\n",
    "        best_path.insert(0, best_tag)\n",
    "    return best_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_sentences, pred_sentences):\n",
    "    \"\"\"Compute overall and tag-wise accuracy.\"\"\"\n",
    "    total = correct = 0\n",
    "    tag_correct = defaultdict(int)\n",
    "    tag_total = defaultdict(int)\n",
    "    for true, pred in zip(true_sentences, pred_sentences):\n",
    "        for (_, true_tag), pred_tag in zip(true, pred):\n",
    "            total += 1\n",
    "            tag_total[true_tag] += 1\n",
    "            if true_tag == pred_tag:\n",
    "                correct += 1\n",
    "                tag_correct[true_tag] += 1\n",
    "    overall_acc = correct / total if total else 0\n",
    "    tag_acc = {tag: tag_correct[tag]/tag_total[tag] for tag in tag_total}\n",
    "    return overall_acc, tag_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 36-Tag Configuration ===\n",
      "Overall Accuracy: 0.7509\n",
      "Tag-wise Accuracy (All 36 Tags):\n",
      "#: 0.0000\n",
      "$: 0.0000\n",
      "'': 0.0000\n",
      "(: 0.0000\n",
      "): 0.0000\n",
      ",: 0.0000\n",
      ".: 0.0000\n",
      ":: 0.8043\n",
      "CC: 0.9822\n",
      "CD: 0.5000\n",
      "DT: 0.9853\n",
      "EX: 0.4167\n",
      "FW: 0.0000\n",
      "IN: 0.9746\n",
      "JJ: 0.6402\n",
      "JJR: 0.4737\n",
      "JJS: 0.0909\n",
      "LS: 0.0000\n",
      "MD: 0.9040\n",
      "NN: 0.9175\n",
      "NNP: 0.5807\n",
      "NNPS: 0.0000\n",
      "NNS: 0.5899\n",
      "PDT: 0.0000\n",
      "POS: 0.0000\n",
      "PRP: 0.8294\n",
      "PRP$: 0.9200\n",
      "RB: 0.4088\n",
      "RBR: 0.0370\n",
      "RBS: 0.0000\n",
      "RP: 0.1522\n",
      "SYM: 0.0000\n",
      "TO: 0.9932\n",
      "UH: 0.0000\n",
      "VB: 0.7237\n",
      "VBD: 0.7344\n",
      "VBG: 0.2577\n",
      "VBN: 0.5372\n",
      "VBP: 0.6079\n",
      "VBZ: 0.7037\n",
      "WDT: 0.6180\n",
      "WP: 0.6429\n",
      "WP$: 0.0000\n",
      "WRB: 0.1765\n",
      "\n",
      "=== 4-Tag Configuration ===\n",
      "Overall Accuracy: 0.8378\n",
      "Tag-wise Accuracy:\n",
      "N: 0.7365\n",
      "V: 0.7733\n",
      "A: 0.6951\n",
      "O: 0.9971\n"
     ]
    }
   ],
   "source": [
    "json_file = 'penn-data.json'  # Replace with your JSON file path\n",
    "sentences = load_data(json_file)\n",
    "train_36, test_36 = split_data(sentences)\n",
    "train_4 = preprocess_4tag(train_36)\n",
    "test_4 = preprocess_4tag(test_36)\n",
    "\n",
    "# Train and evaluate 36-tag model\n",
    "initial_36, trans_36, emit_36, mft_36, tags_36, words_36 = train_model_a(train_36)\n",
    "preds_36 = [viterbi_a(s, initial_36, trans_36, emit_36, mft_36, tags_36, words_36) for s in test_36]\n",
    "acc_36, tag_acc_36 = evaluate(test_36, preds_36)\n",
    "\n",
    "# Train and evaluate 4-tag model\n",
    "initial_4, trans_4, emit_4, mft_4, tags_4, words_4 = train_model_a(train_4)\n",
    "preds_4 = [viterbi_a(s, initial_4, trans_4, emit_4, mft_4, tags_4, words_4) for s in test_4]\n",
    "acc_4, tag_acc_4 = evaluate(test_4, preds_4)\n",
    "\n",
    "# Print results\n",
    "print(\"=== 36-Tag Configuration ===\")\n",
    "print(f\"Overall Accuracy: {acc_36:.4f}\")\n",
    "print(\"Tag-wise Accuracy (All 36 Tags):\")\n",
    "\n",
    "# List of all 36 Penn Treebank tags (sorted alphabetically)\n",
    "all_36_tags = [\n",
    "    '#', '$', \"''\", '(', ')', ',', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', \n",
    "    'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', \n",
    "    'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', \n",
    "    'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'\n",
    "]\n",
    "\n",
    "# Print accuracy for all 36 tags (including those not in the test set)\n",
    "for tag in sorted(all_36_tags):\n",
    "    acc = tag_acc_36.get(tag, 0.0)  # Default to 0.0 if tag not in test set\n",
    "    print(f\"{tag}: {acc:.4f}\")\n",
    "\n",
    "print(\"\\n=== 4-Tag Configuration ===\")\n",
    "print(f\"Overall Accuracy: {acc_4:.4f}\")\n",
    "print(\"Tag-wise Accuracy:\")\n",
    "for tag in ['N', 'V', 'A', 'O']:\n",
    "    print(f\"{tag}: {tag_acc_4.get(tag, 0.0):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
