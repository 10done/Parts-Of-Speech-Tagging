{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(json_file):\n",
    "    \"\"\"Load raw data from JSON file.\"\"\"\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return [\n",
    "        (sentence.split(), tags) \n",
    "        for sentence, tags in data\n",
    "    ]\n",
    "\n",
    "def split_data(sentences, test_ratio=0.2):\n",
    "    \"\"\"Split data into train/test sets.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.permutation(len(sentences))\n",
    "    split_idx = int(len(sentences) * (1 - test_ratio))\n",
    "    return [sentences[i] for i in indices[:split_idx]], [sentences[i] for i in indices[split_idx:]]\n",
    "\n",
    "def map_to_4tags(tag):\n",
    "    \"\"\"Collapse 36 tags into 4 categories.\"\"\"\n",
    "    if tag.startswith('N'): return 'N'\n",
    "    elif tag.startswith('V'): return 'V'\n",
    "    elif tag.startswith('JJ') or tag.startswith('RB'): return 'A'\n",
    "    else: return 'O'\n",
    "\n",
    "def preprocess_4tag(sentences):\n",
    "    \"\"\"Convert tags in sentences to 4 categories.\"\"\"\n",
    "    return [\n",
    "        ([word.lower() for word in words], [map_to_4tags(tag) for tag in tags])\n",
    "        for (words, tags) in sentences\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mappings(sentences):\n",
    "    \"\"\"Generate tag/word indices from training data.\"\"\"\n",
    "    all_tags, all_words = set(), set()\n",
    "    for words, tags in sentences:\n",
    "        all_tags.update(tags)\n",
    "        all_words.update(words)\n",
    "    tag2idx = {tag: i for i, tag in enumerate(all_tags)}\n",
    "    word2idx = {word: i for i, word in enumerate(all_words)}\n",
    "    idx2tag = {i: tag for tag, i in tag2idx.items()}\n",
    "    return tag2idx, word2idx, idx2tag, all_tags, all_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, tag2idx, word2idx):\n",
    "    \"\"\"Train HMM with numpy matrices (add-1 smoothing).\"\"\"\n",
    "    num_tags = len(tag2idx)\n",
    "    num_words = len(word2idx)\n",
    "    \n",
    "    # Initialize matrices\n",
    "    transition = np.ones((num_tags, num_tags))\n",
    "    initial = np.ones(num_tags)\n",
    "    emission = np.ones((num_tags, num_words))\n",
    "    \n",
    "    for words, tags in train_data:\n",
    "        prev_tag_idx = None\n",
    "        for i, (word, tag) in enumerate(zip(words, tags)):\n",
    "            word_idx = word2idx.get(word.lower(), -1)\n",
    "            tag_idx = tag2idx[tag]\n",
    "            \n",
    "            if i == 0:\n",
    "                initial[tag_idx] += 1\n",
    "            else:\n",
    "                transition[prev_tag_idx, tag_idx] += 1\n",
    "            \n",
    "            if word_idx != -1:\n",
    "                emission[tag_idx, word_idx] += 1\n",
    "            \n",
    "            prev_tag_idx = tag_idx\n",
    "    \n",
    "    # Normalize probabilities\n",
    "    initial = np.log(initial / initial.sum())\n",
    "    transition = np.log(transition / transition.sum(axis=1, keepdims=True))\n",
    "    emission = np.log(emission / emission.sum(axis=1, keepdims=True))\n",
    "    \n",
    "    return initial, transition, emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sentence_words, initial, transition, emission, tag2idx, word2idx):\n",
    "    \"\"\"Fast Viterbi decoding with numpy.\"\"\"\n",
    "    n = len(sentence_words)\n",
    "    num_tags = len(tag2idx)\n",
    "    \n",
    "    # Precompute word indices\n",
    "    word_indices = [word2idx.get(word.lower(), -1) for word in sentence_words]\n",
    "    \n",
    "    # Initialize DP tables\n",
    "    viterbi = np.zeros((n, num_tags)) + initial\n",
    "    backpointers = np.zeros((n, num_tags), dtype=int)\n",
    "    \n",
    "    # First word\n",
    "    if word_indices[0] != -1:\n",
    "        viterbi[0] += emission[:, word_indices[0]]\n",
    "    \n",
    "    # Iterate\n",
    "    for t in range(1, n):\n",
    "        emit = emission[:, word_indices[t]] if word_indices[t] != -1 else 0\n",
    "        scores = viterbi[t-1][:, None] + transition + emit\n",
    "        viterbi[t] = np.max(scores, axis=0)\n",
    "        backpointers[t] = np.argmax(scores, axis=0)\n",
    "    \n",
    "    # Backtrack\n",
    "    best_path = [np.argmax(viterbi[-1])]\n",
    "    for t in reversed(range(1, n)):\n",
    "        best_path.insert(0, backpointers[t, best_path[0]])\n",
    "    \n",
    "    return best_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_probabilities(matrix, idx2tag, top_k=5, is_transition=False):\n",
    "    \"\"\"Extract top probabilities from numpy matrix.\"\"\"\n",
    "    results = {}\n",
    "    for i in range(matrix.shape[0]):\n",
    "        tag = idx2tag[i]\n",
    "        probs = matrix[i]\n",
    "        top_indices = np.argsort(probs)[-top_k:][::-1]\n",
    "        results[tag] = [\n",
    "            (idx2tag[j] if is_transition else j, np.exp(probs[j]))  # Fixed parenthesis\n",
    "            for j in top_indices\n",
    "        ]\n",
    "    return results\n",
    "\n",
    "def print_tag_probabilities(initial_probs, transition_probs, emission_probs, \n",
    "                           idx2tag, word2idx, top_k=5):\n",
    "    \"\"\"Print probabilities in readable format.\"\"\"\n",
    "    # Reverse word index for lookup\n",
    "    idx2word = {v: k for k, v in word2idx.items()}\n",
    "    \n",
    "    print(\"\\n=== Initial Probabilities ===\")\n",
    "    initial_exp = np.exp(initial_probs)\n",
    "    for tag_idx in np.argsort(initial_exp)[::-1][:top_k]:\n",
    "        print(f\"{idx2tag[tag_idx]}: {initial_exp[tag_idx]:.4f}\")\n",
    "    \n",
    "    print(\"\\n=== Top Transition Probabilities ===\")\n",
    "    trans_top = get_top_probabilities(np.exp(transition_probs), idx2tag, top_k, True)\n",
    "    for tag, probs in list(trans_top.items())[:3]:  # Print first 3 tags for brevity\n",
    "        print(f\"From {tag}:\")\n",
    "        for target_tag, prob in probs:\n",
    "            print(f\"  → {target_tag}: {prob:.4f}\")\n",
    "    \n",
    "    print(\"\\n=== Top Emission Probabilities ===\")\n",
    "    emit_top = get_top_probabilities(np.exp(emission_probs), idx2tag, top_k)\n",
    "    for tag, probs in list(emit_top.items())[:3]:  # Print first 3 tags\n",
    "        print(f\"Tag {tag}:\")\n",
    "        for word_idx, prob in probs:\n",
    "            word = idx2word.get(word_idx, \"UNK\")\n",
    "            print(f\"  - {word}: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 36-tag model...\n",
      "36-Tag Accuracy: 0.7286\n",
      "\n",
      "Training 4-tag model...\n",
      "4-Tag Accuracy: 0.8821\n"
     ]
    }
   ],
   "source": [
    "sentences = load_data(\"penn-data.json\")\n",
    "train_36, test_36 = split_data(sentences)\n",
    "\n",
    "# Preprocess for 4-tag\n",
    "train_4 = preprocess_4tag(train_36)\n",
    "test_4 = preprocess_4tag(test_36)\n",
    "\n",
    "# ------------------- 36-Tag Configuration ------------------- #\n",
    "print(\"Training 36-tag model...\")\n",
    "tag2idx_36, word2idx_36, idx2tag_36, _, _ = create_mappings(train_36)\n",
    "initial_36, trans_36, emit_36 = train_model(train_36, tag2idx_36, word2idx_36)\n",
    "\n",
    "# Predict\n",
    "preds_36 = []\n",
    "for words, _ in test_36:\n",
    "    path = viterbi(words, initial_36, trans_36, emit_36, tag2idx_36, word2idx_36)\n",
    "    preds_36.append([idx2tag_36[idx] for idx in path])\n",
    "\n",
    "# Evaluate\n",
    "correct_36 = sum(1 for (_, tags), pred in zip(test_36, preds_36) for t, p in zip(tags, pred) if t == p)\n",
    "total_36 = sum(len(tags) for (_, tags) in test_36)\n",
    "print(f\"36-Tag Accuracy: {correct_36 / total_36:.4f}\")\n",
    "\n",
    "# ------------------- 4-Tag Configuration ------------------- #\n",
    "print(\"\\nTraining 4-tag model...\")\n",
    "tag2idx_4, word2idx_4, idx2tag_4, _, _ = create_mappings(train_4)\n",
    "initial_4, trans_4, emit_4 = train_model(train_4, tag2idx_4, word2idx_4)\n",
    "\n",
    "# Predict\n",
    "preds_4 = []\n",
    "for words, _ in test_4:\n",
    "    path = viterbi(words, initial_4, trans_4, emit_4, tag2idx_4, word2idx_4)\n",
    "    preds_4.append([idx2tag_4[idx] for idx in path])\n",
    "\n",
    "# Evaluate\n",
    "correct_4 = sum(1 for (_, tags), pred in zip(test_4, preds_4) for t, p in zip(tags, pred) if t == p)\n",
    "total_4 = sum(len(tags) for (_, tags) in test_4)\n",
    "print(f\"4-Tag Accuracy: {correct_4 / total_4:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "36-Tag Model Probabilities\n",
      "========================================\n",
      "\n",
      "=== Initial Probabilities ===\n",
      "DT: 0.2448\n",
      "NNP: 0.1981\n",
      "IN: 0.1363\n",
      "PRP: 0.0861\n",
      "CC: 0.0536\n",
      "\n",
      "=== Top Transition Probabilities ===\n",
      "From WP$:\n",
      "  → NN: 1.1503\n",
      "  → NNS: 1.1052\n",
      "  → JJ: 1.0408\n",
      "  → JJS: 1.0202\n",
      "  → CD: 1.0202\n",
      "From VBG:\n",
      "  → DT: 1.2077\n",
      "  → NN: 1.1523\n",
      "  → IN: 1.1415\n",
      "  → NNS: 1.0976\n",
      "  → TO: 1.0919\n",
      "From CC:\n",
      "  → NNP: 1.1578\n",
      "  → NN: 1.1277\n",
      "  → DT: 1.1204\n",
      "  → JJ: 1.1108\n",
      "  → NNS: 1.0697\n",
      "\n",
      "=== Top Emission Probabilities ===\n",
      "Tag WP$:\n",
      "  - whose: 1.0008\n",
      "  - corkscrews,: 1.0001\n",
      "  - bills.: 1.0001\n",
      "  - map: 1.0001\n",
      "  - subscribers,: 1.0001\n",
      "Tag VBG:\n",
      "  - according: 1.0022\n",
      "  - being: 1.0019\n",
      "  - including: 1.0019\n",
      "  - growing: 1.0013\n",
      "  - trying: 1.0012\n",
      "Tag CC:\n",
      "  - and: 1.0786\n",
      "  - or: 1.0151\n",
      "  - but: 1.0145\n",
      "  - &: 1.0042\n",
      "  - and,: 1.0007\n",
      "\n",
      "========================================\n",
      "4-Tag Model Probabilities\n",
      "========================================\n",
      "\n",
      "=== Initial Probabilities ===\n",
      "O: 0.5764\n",
      "N: 0.2957\n",
      "A: 0.0986\n",
      "V: 0.0293\n",
      "\n",
      "=== Top Transition Probabilities ===\n",
      "From O:\n",
      "  → N: 1.4569\n",
      "  → O: 1.3708\n",
      "  → V: 1.1768\n",
      "  → A: 1.1566\n",
      "From V:\n",
      "  → O: 1.7306\n",
      "  → N: 1.1651\n",
      "  → V: 1.1638\n",
      "  → A: 1.1584\n",
      "From N:\n",
      "  → O: 1.5032\n",
      "  → N: 1.4347\n",
      "  → V: 1.1957\n",
      "  → A: 1.0541\n",
      "\n",
      "=== Top Emission Probabilities ===\n",
      "Tag O:\n",
      "  - the: 1.1017\n",
      "  - of: 1.0489\n",
      "  - to: 1.0468\n",
      "  - a: 1.0420\n",
      "  - in: 1.0366\n",
      "Tag V:\n",
      "  - is: 1.0219\n",
      "  - said: 1.0177\n",
      "  - was: 1.0121\n",
      "  - be: 1.0119\n",
      "  - are: 1.0115\n",
      "Tag N:\n",
      "  - mr.: 1.0087\n",
      "  - u.s.: 1.0045\n",
      "  - new: 1.0038\n",
      "  - company: 1.0036\n",
      "  - stock: 1.0034\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"36-Tag Model Probabilities\")\n",
    "print(\"=\"*40)\n",
    "print_tag_probabilities(\n",
    "    initial_36, \n",
    "    trans_36, \n",
    "    emit_36, \n",
    "    idx2tag_36, \n",
    "    word2idx_36\n",
    ")\n",
    "\n",
    "# ------------------- 4-Tag Probabilities ------------------- #\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"4-Tag Model Probabilities\")\n",
    "print(\"=\"*40)\n",
    "print_tag_probabilities(\n",
    "    initial_4, \n",
    "    trans_4, \n",
    "    emit_4, \n",
    "    idx2tag_4, \n",
    "    word2idx_4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.017905138734850074\n",
      "Per-Tag Accuracy: {'DT': 0.002482929857231533, 'JJS': 0.0, 'JJ': 0.0, 'NNS': 0.0, 'VBD': 0.0, 'VBN': 0.0, 'IN': 0.0025614754098360654, 'CD': 0.0009689922480620155, 'TO': 0.0, 'VB': 0.0, 'NN': 0.0028735632183908046, 'NNP': 0.9855555555555555, 'RB': 0.0, 'JJR': 0.0, 'NNPS': 0.0, 'RBS': 0.0, 'PRP$': 0.0, 'VBP': 0.0, 'CC': 0.0, 'VBZ': 0.0, 'VBG': 0.0, 'PRP': 0.0, 'MD': 0.0, 'RP': 0.0, 'EX': 0.0, 'WDT': 0.0, ':': 0.0, '#': 0.0, '-LRB-': 0.0, '-RRB-': 0.0, 'WP': 0.0, 'RBR': 0.0, 'WRB': 0.0, 'PDT': 0.0, \"''\": 0.0, 'WP$': 0.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class HMMPOSTagger:\n",
    "    def __init__(self, order=1):\n",
    "        self.order = order\n",
    "        self.tag_counts = Counter()\n",
    "        self.word_tag_counts = defaultdict(Counter)\n",
    "        self.transitions = defaultdict(Counter)\n",
    "        self.unique_words = set()\n",
    "        self.smoothing = 1e-5\n",
    "        self.default_tag = None\n",
    "    \n",
    "    def train(self, sentences):\n",
    "        for words, tags in sentences:\n",
    "            prev_tags = tuple(['<s>'] * self.order)\n",
    "            for word, tag in zip(words, tags):\n",
    "                self.tag_counts[tag] += 1\n",
    "                self.word_tag_counts[word][tag] += 1\n",
    "                self.transitions[prev_tags][tag] += 1\n",
    "                prev_tags = (prev_tags + (tag,))[-self.order:]\n",
    "                self.unique_words.add(word)\n",
    "        self.default_tag = max(self.tag_counts, key=self.tag_counts.get)\n",
    "    \n",
    "    def compute_probabilities(self):\n",
    "        self.tag_totals = {tag: sum(self.transitions[prev].values()) for prev in self.transitions for tag in self.transitions[prev]}\n",
    "        self.word_totals = {word: sum(self.word_tag_counts[word].values()) for word in self.word_tag_counts}\n",
    "    \n",
    "    def viterbi(self, sentence):\n",
    "        n, states = len(sentence), list(self.tag_counts.keys())\n",
    "        viterbi = np.full((len(states), n), -np.inf)\n",
    "        backpointer = np.zeros((len(states), n), dtype=int)\n",
    "        \n",
    "        for i, tag in enumerate(states):\n",
    "            viterbi[i, 0] = np.log((self.tag_counts[tag] + self.smoothing) / sum(self.tag_counts.values()))\n",
    "        \n",
    "        for t in range(1, n):\n",
    "            for i, tag in enumerate(states):\n",
    "                max_prob, best_prev = -np.inf, 0\n",
    "                for j, prev_tag in enumerate(states):\n",
    "                    trans_prob = np.log((self.transitions[(prev_tag,)][tag] + self.smoothing) / (self.tag_counts[prev_tag] + self.smoothing * len(states)))\n",
    "                    word_prob = np.log((self.word_tag_counts[sentence[t]][tag] + self.smoothing) / (self.tag_counts[tag] + self.smoothing * len(self.unique_words)))\n",
    "                    prob = viterbi[j, t-1] + trans_prob + word_prob\n",
    "                    if prob > max_prob:\n",
    "                        max_prob, best_prev = prob, j\n",
    "                viterbi[i, t] = max_prob\n",
    "                backpointer[i, t] = best_prev\n",
    "        \n",
    "        best_last = np.argmax(viterbi[:, -1])\n",
    "        best_path = [states[best_last]]\n",
    "        for t in range(n - 1, 0, -1):\n",
    "            best_last = backpointer[best_last, t]\n",
    "            best_path.insert(0, states[best_last])\n",
    "        return best_path\n",
    "    \n",
    "    def evaluate(self, test_sentences):\n",
    "        correct, total = 0, 0\n",
    "        per_tag_correct, per_tag_total = Counter(), Counter()\n",
    "        for words, true_tags in test_sentences:\n",
    "            predicted_tags = self.viterbi(words)\n",
    "            for true, pred in zip(true_tags, predicted_tags):\n",
    "                if true == pred:\n",
    "                    correct += 1\n",
    "                    per_tag_correct[true] += 1\n",
    "                per_tag_total[true] += 1\n",
    "            total += len(words)\n",
    "        accuracy = correct / total\n",
    "        per_tag_accuracy = {tag: per_tag_correct[tag] / per_tag_total[tag] for tag in per_tag_total}\n",
    "        return accuracy, per_tag_accuracy\n",
    "\n",
    "# Load dataset\n",
    "with open(\"penn-data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Split into train and test\n",
    "split_idx = int(0.8 * len(data))\n",
    "train_data, test_data = data[:split_idx], data[split_idx:]\n",
    "\n",
    "# Train and evaluate model\n",
    "tagger = HMMPOSTagger(order=1)\n",
    "tagger.train(train_data)\n",
    "tagger.compute_probabilities()\n",
    "accuracy, per_tag_acc = tagger.evaluate(test_data)\n",
    "print(\"Overall Accuracy:\", accuracy)\n",
    "print(\"Per-Tag Accuracy:\", per_tag_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3914 sentences from the dataset.\n",
      "36-Tag Model: 3131 training sentences, 783 test sentences.\n",
      "4-Tag Model: Collapsed training and test data created.\n",
      "\n",
      "--- 36-Tag Model Evaluation ---\n",
      "Overall Accuracy: 0.4774899320791008\n",
      "Per-Tag Accuracy:\n",
      "  #: 0.00\n",
      "  '': 0.00\n",
      "  -LRB-: 0.00\n",
      "  -RRB-: 0.07\n",
      "  :: 0.00\n",
      "  CC: 0.20\n",
      "  CD: 0.22\n",
      "  DT: 0.77\n",
      "  EX: 0.32\n",
      "  IN: 0.76\n",
      "  JJ: 0.39\n",
      "  JJR: 0.02\n",
      "  JJS: 0.00\n",
      "  LS: 0.00\n",
      "  MD: 0.38\n",
      "  NN: 0.73\n",
      "  NNP: 0.47\n",
      "  NNPS: 0.08\n",
      "  NNS: 0.21\n",
      "  PDT: 0.00\n",
      "  PRP: 0.46\n",
      "  PRP$: 0.29\n",
      "  RB: 0.11\n",
      "  RBR: 0.04\n",
      "  RBS: 0.00\n",
      "  RP: 0.00\n",
      "  TO: 0.50\n",
      "  VB: 0.55\n",
      "  VBD: 0.26\n",
      "  VBG: 0.03\n",
      "  VBN: 0.23\n",
      "  VBP: 0.20\n",
      "  VBZ: 0.21\n",
      "  WDT: 0.07\n",
      "  WP: 0.05\n",
      "  WP$: 0.00\n",
      "  WRB: 0.11\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\IPython\\core\\interactiveshell.py\", line 2170, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "        etype, value, tb, tb_offset=tb_offset\n",
      "    )\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\IPython\\core\\ultratb.py\", line 1457, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self, etype, evalue, etb, tb_offset, number_of_lines_of_context\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\IPython\\core\\ultratb.py\", line 1348, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\IPython\\core\\ultratb.py\", line 1195, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                                                           tb_offset)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\IPython\\core\\ultratb.py\", line 1085, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\IPython\\core\\ultratb.py\", line 1182, in get_records\n",
      "    res = list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "          ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\stack_data\\core.py\", line 597, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "    ...<4 lines>...\n",
      "    )\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\stack_data\\utils.py\", line 83, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\stack_data\\core.py\", line 587, in mapper\n",
      "    return cls(f, options)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\stack_data\\core.py\", line 551, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "                     ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\executing\\executing.py\", line 264, in executing\n",
      "    source = cls.for_frame(frame)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\executing\\executing.py\", line 183, in for_frame\n",
      "    return cls.for_filename(frame.f_code.co_filename, frame.f_globals or {}, use_cache)\n",
      "           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\executing\\executing.py\", line 212, in for_filename\n",
      "    return cls._for_filename_and_lines(filename, tuple(lines))\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\executing\\executing.py\", line 223, in _for_filename_and_lines\n",
      "    result = source_cache[(filename, lines)] = cls(filename, lines)\n",
      "                                               ~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\executing\\executing.py\", line 163, in __init__\n",
      "    self.tree = ast.parse(self.text, filename=filename)\n",
      "                ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.752.0_x64__qbz5n2kfra8p0\\Lib\\ast.py\", line 54, in parse\n",
      "    return compile(source, filename, mode, flags,\n",
      "                   _feature_version=feature_version, optimize=optimize)\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----- Data Loading from JSON -----\n",
    "\n",
    "def load_data_json(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a JSON file with the following format:\n",
    "    [\n",
    "      [\"Sentence string\", [\"TAG1\", \"TAG2\", ...]],\n",
    "      ...\n",
    "    ]\n",
    "    Returns a list of sentences where each sentence is a list of (word, tag) tuples.\n",
    "    Tokenizes the sentence by splitting on whitespace.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        for entry in json_data:\n",
    "            sentence_str, tags = entry\n",
    "            tokens = sentence_str.strip().split()\n",
    "            if len(tokens) != len(tags):\n",
    "                print(\"Warning: token count and tag count mismatch for sentence:\", sentence_str)\n",
    "                continue  # Skip sentences with mismatch\n",
    "            data.append(list(zip(tokens, tags)))\n",
    "    return data\n",
    "\n",
    "def split_data(data, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Shuffle and split data into training and test sets.\n",
    "    \"\"\"\n",
    "    random.shuffle(data)\n",
    "    split_index = int(len(data) * train_ratio)\n",
    "    return data[:split_index], data[split_index:]\n",
    "\n",
    "# ----- Tag Collapsing for 4-Tag Configuration -----\n",
    "\n",
    "def collapse_tag(tag):\n",
    "    \"\"\"\n",
    "    Collapse a given tag into one of four categories:\n",
    "      N -> noun tags (NN, NNS, NNP, NNPS)\n",
    "      V -> verb tags (VB, VBD, VBG, VBN, VBP, VBZ)\n",
    "      A -> adjectives and adverbs (JJ, JJR, JJS, RB, RBR, RBS, WRB)\n",
    "      O -> all others\n",
    "    \"\"\"\n",
    "    noun_tags = {'NN', 'NNS', 'NNP', 'NNPS'}\n",
    "    verb_tags = {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}\n",
    "    adj_tags  = {'JJ', 'JJR', 'JJS'}\n",
    "    adv_tags  = {'RB', 'RBR', 'RBS', 'WRB'}\n",
    "    if tag in noun_tags:\n",
    "        return \"N\"\n",
    "    elif tag in verb_tags:\n",
    "        return \"V\"\n",
    "    elif tag in adj_tags or tag in adv_tags:\n",
    "        return \"A\"\n",
    "    else:\n",
    "        return \"O\"\n",
    "\n",
    "def collapse_data(data):\n",
    "    \"\"\"\n",
    "    Given a dataset (list of sentences with (word, tag) tuples),\n",
    "    return a new dataset where each tag is collapsed using collapse_tag.\n",
    "    \"\"\"\n",
    "    collapsed = []\n",
    "    for sentence in data:\n",
    "        new_sentence = [(word, collapse_tag(tag)) for word, tag in sentence]\n",
    "        collapsed.append(new_sentence)\n",
    "    return collapsed\n",
    "\n",
    "# ----- Training the HMM (Configuration C) with Smoothing -----\n",
    "# Emission probability: P(word | tag, previous_word)\n",
    "# Transition probability: P(curr_tag | prev_tag)\n",
    "# We'll use add-one (Laplace) smoothing with a smoothing parameter alpha.\n",
    "\n",
    "def train_hmm(train_data, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Train a first-order HMM with smoothing.\n",
    "    Returns:\n",
    "      - init_probs: smoothed initial probabilities P(tag as first tag)\n",
    "      - trans_probs: smoothed transition probabilities P(curr_tag | prev_tag)\n",
    "      - emission_probs: smoothed emission probabilities P(word | tag, previous_word)\n",
    "      - vocab: set of observed words\n",
    "      - default_tag: most frequent tag (used for unseen words)\n",
    "    \"\"\"\n",
    "    init_counts = Counter()\n",
    "    trans_counts = defaultdict(Counter)\n",
    "    emission_counts = defaultdict(lambda: defaultdict(int))  # key: (tag, previous_word)\n",
    "    tag_prev_counts = defaultdict(int)  # total count for each (tag, previous_word)\n",
    "    tag_counts = Counter()\n",
    "    vocab = set()\n",
    "\n",
    "    for sentence in train_data:\n",
    "        prev_word = \"<s>\"  # Special symbol for sentence start.\n",
    "        if not sentence:\n",
    "            continue\n",
    "        first_word, first_tag = sentence[0]\n",
    "        init_counts[first_tag] += 1\n",
    "        tag_counts[first_tag] += 1\n",
    "        vocab.add(first_word)\n",
    "        # Emission for first word using \"<s>\" as previous word.\n",
    "        emission_counts[(first_tag, prev_word)][first_word] += 1\n",
    "        tag_prev_counts[(first_tag, prev_word)] += 1\n",
    "\n",
    "        prev_tag = first_tag\n",
    "        # Process the rest of the sentence.\n",
    "        for i in range(1, len(sentence)):\n",
    "            word, tag = sentence[i]\n",
    "            vocab.add(word)\n",
    "            trans_counts[prev_tag][tag] += 1\n",
    "            tag_counts[tag] += 1\n",
    "            prev_word = sentence[i-1][0]\n",
    "            emission_counts[(tag, prev_word)][word] += 1\n",
    "            tag_prev_counts[(tag, prev_word)] += 1\n",
    "            prev_tag = tag\n",
    "\n",
    "    num_sentences = len(train_data)\n",
    "    states = list(tag_counts.keys())\n",
    "    num_states = len(states)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Smoothing for initial probabilities.\n",
    "    init_probs = {}\n",
    "    for tag in states:\n",
    "        init_probs[tag] = (init_counts.get(tag, 0) + alpha) / (num_sentences + alpha * num_states)\n",
    "\n",
    "    # Smoothing for transition probabilities.\n",
    "    trans_probs = {}\n",
    "    for prev_tag in states:\n",
    "        total = sum(trans_counts[prev_tag].values())\n",
    "        trans_probs[prev_tag] = {}\n",
    "        for curr_tag in states:\n",
    "            count = trans_counts[prev_tag].get(curr_tag, 0)\n",
    "            trans_probs[prev_tag][curr_tag] = (count + alpha) / (total + alpha * num_states)\n",
    "    for tag in states:\n",
    "        if tag not in trans_probs:\n",
    "            trans_probs[tag] = {curr_tag: 1/num_states for curr_tag in states}\n",
    "\n",
    "    # Smoothing for emission probabilities.\n",
    "    emission_probs = {}\n",
    "    for key in emission_counts:\n",
    "        total = tag_prev_counts[key]\n",
    "        emission_probs[key] = {}\n",
    "        for word in vocab:\n",
    "            count = emission_counts[key].get(word, 0)\n",
    "            emission_probs[key][word] = (count + alpha) / (total + alpha * vocab_size)\n",
    "    # For any (tag, prev_word) key not seen during training, no entry will be in emission_probs.\n",
    "\n",
    "    default_tag = tag_counts.most_common(1)[0][0]\n",
    "    return init_probs, trans_probs, emission_probs, vocab, default_tag\n",
    "\n",
    "# ----- Viterbi Decoding -----\n",
    "\n",
    "def viterbi(sentence, states, init_probs, trans_probs, emission_probs, vocab, default_tag):\n",
    "    \"\"\"\n",
    "    Apply the Viterbi algorithm to find the best tag sequence for the given sentence.\n",
    "    Uses emission probability: P(word | current_tag, previous_word).\n",
    "    For the first word, previous word is \"<s>\".\n",
    "    For unseen words, the default_tag is used (you can later extend this with an unknown-word model).\n",
    "    \"\"\"\n",
    "    n = len(sentence)\n",
    "    dp = [{} for _ in range(n)]          # dp[i][state]: best log-probability ending with 'state' at position i\n",
    "    backpointer = [{} for _ in range(n)]\n",
    "    \n",
    "    # Initialization for the first word.\n",
    "    first_word = sentence[0]\n",
    "    prev_obs = \"<s>\"\n",
    "    if first_word not in vocab:\n",
    "        # For unseen first word, force default_tag.\n",
    "        for state in states:\n",
    "            if state == default_tag:\n",
    "                dp[0][state] = math.log(init_probs.get(state, 1e-10))\n",
    "            else:\n",
    "                dp[0][state] = -float('inf')\n",
    "            backpointer[0][state] = None\n",
    "    else:\n",
    "        for state in states:\n",
    "            emis_prob = emission_probs.get((state, prev_obs), {}).get(first_word, 1e-10)\n",
    "            dp[0][state] = math.log(init_probs.get(state, 1e-10)) + math.log(emis_prob)\n",
    "            backpointer[0][state] = None\n",
    "\n",
    "    # Recursion.\n",
    "    for i in range(1, n):\n",
    "        word = sentence[i]\n",
    "        prev_obs = sentence[i-1]  # previous observed word\n",
    "        if word not in vocab:\n",
    "            # For unseen word, use a fallback approach: here we assign the default_tag a pseudo probability.\n",
    "            for curr_state in states:\n",
    "                if curr_state == default_tag:\n",
    "                    best_prob = -float('inf')\n",
    "                    best_prev = None\n",
    "                    for prev_state in states:\n",
    "                        trans_prob = trans_probs.get(prev_state, {}).get(curr_state, 1e-10)\n",
    "                        prob = dp[i-1][prev_state] + math.log(trans_prob)\n",
    "                        if prob > best_prob:\n",
    "                            best_prob = prob\n",
    "                            best_prev = prev_state\n",
    "                    if best_prev is None:\n",
    "                        best_prev = default_tag\n",
    "                    dp[i][curr_state] = best_prob\n",
    "                    backpointer[i][curr_state] = best_prev\n",
    "                else:\n",
    "                    dp[i][curr_state] = -float('inf')\n",
    "                    backpointer[i][curr_state] = None\n",
    "            continue\n",
    "\n",
    "        for curr_state in states:\n",
    "            emis_prob = emission_probs.get((curr_state, prev_obs), {}).get(word, 1e-10)\n",
    "            best_prob = -float('inf')\n",
    "            best_prev = None\n",
    "            for prev_state in states:\n",
    "                trans_prob = trans_probs.get(prev_state, {}).get(curr_state, 1e-10)\n",
    "                prob = dp[i-1][prev_state] + math.log(trans_prob) + math.log(emis_prob)\n",
    "                if prob > best_prob:\n",
    "                    best_prob = prob\n",
    "                    best_prev = prev_state\n",
    "            if best_prev is None:\n",
    "                best_prev = default_tag\n",
    "            dp[i][curr_state] = best_prob\n",
    "            backpointer[i][curr_state] = best_prev\n",
    "\n",
    "    # Backtrace to recover the best path.\n",
    "    best_last_state = max(dp[n-1], key=dp[n-1].get)\n",
    "    best_path = [best_last_state]\n",
    "    for i in range(n-1, 0, -1):\n",
    "        prev_state = backpointer[i][best_path[0]]\n",
    "        if prev_state is None:\n",
    "            prev_state = default_tag\n",
    "        best_path.insert(0, prev_state)\n",
    "    \n",
    "    return best_path\n",
    "\n",
    "# ----- Evaluation Functions -----\n",
    "\n",
    "def evaluate(test_data, states, init_probs, trans_probs, emission_probs, vocab, default_tag):\n",
    "    \"\"\"\n",
    "    Evaluate the HMM tagger on test_data.\n",
    "    Returns the overall tagging accuracy.\n",
    "    \"\"\"\n",
    "    total_tags = 0\n",
    "    correct_tags = 0\n",
    "    for sentence in test_data:\n",
    "        words = [word for word, tag in sentence]\n",
    "        gold_tags = [tag for word, tag in sentence]\n",
    "        pred_tags = viterbi(words, states, init_probs, trans_probs, emission_probs, vocab, default_tag)\n",
    "        for gold, pred in zip(gold_tags, pred_tags):\n",
    "            total_tags += 1\n",
    "            if gold == pred:\n",
    "                correct_tags += 1\n",
    "    return correct_tags / total_tags if total_tags > 0 else 0\n",
    "\n",
    "def per_tag_accuracy(test_data, states, init_probs, trans_probs, emission_probs, vocab, default_tag):\n",
    "    \"\"\"\n",
    "    Compute per-tag accuracy.\n",
    "    Returns a dictionary mapping each tag to its accuracy.\n",
    "    \"\"\"\n",
    "    tag_correct = Counter()\n",
    "    tag_total = Counter()\n",
    "    \n",
    "    for sentence in test_data:\n",
    "        words = [word for word, tag in sentence]\n",
    "        gold_tags = [tag for word, tag in sentence]\n",
    "        pred_tags = viterbi(words, states, init_probs, trans_probs, emission_probs, vocab, default_tag)\n",
    "        for gold, pred in zip(gold_tags, pred_tags):\n",
    "            tag_total[gold] += 1\n",
    "            if gold == pred:\n",
    "                tag_correct[gold] += 1\n",
    "                \n",
    "    return { tag: tag_correct[tag] / tag_total[tag] for tag in tag_total }\n",
    "\n",
    "# ----- Main Execution -----\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset from JSON.\n",
    "    data = load_data_json(\"penn-data.json\")\n",
    "    print(f\"Loaded {len(data)} sentences from the dataset.\")\n",
    "\n",
    "    # Split the data (80:20) for the 36-tag configuration.\n",
    "    train_data_36, test_data_36 = split_data(data, train_ratio=0.8)\n",
    "    print(f\"36-Tag Model: {len(train_data_36)} training sentences, {len(test_data_36)} test sentences.\")\n",
    "    \n",
    "    # Create collapsed (4-tag) versions for training and testing.\n",
    "    train_data_4 = collapse_data(train_data_36)\n",
    "    test_data_4 = collapse_data(test_data_36)\n",
    "    print(\"4-Tag Model: Collapsed training and test data created.\")\n",
    "\n",
    "    # ------------------- 36-Tag Model -------------------\n",
    "    # Train the HMM on the original 36-tag data.\n",
    "    init_probs_36, trans_probs_36, emission_probs_36, vocab_36, default_tag_36 = train_hmm(train_data_36, alpha=0.5)\n",
    "    states_36 = list({ tag for sentence in train_data_36 for _, tag in sentence })\n",
    "    \n",
    "    overall_acc_36 = evaluate(test_data_36, states_36, init_probs_36, trans_probs_36, emission_probs_36, vocab_36, default_tag_36)\n",
    "    tag_acc_36 = per_tag_accuracy(test_data_36, states_36, init_probs_36, trans_probs_36, emission_probs_36, vocab_36, default_tag_36)\n",
    "    \n",
    "    print(\"\\n--- 36-Tag Model Evaluation ---\")\n",
    "    print(\"Overall Accuracy:\", overall_acc_36)\n",
    "    print(\"Per-Tag Accuracy:\")\n",
    "    for tag, acc in sorted(tag_acc_36.items()):\n",
    "        print(f\"  {tag}: {acc:.2f}\")\n",
    "\n",
    "    # ------------------- 4-Tag Model -------------------\n",
    "    # Train the HMM on the collapsed 4-tag data.\n",
    "    init_probs_4, trans_probs_4, emission_probs_4, vocab_4, default_tag_4 = train_hmm(train_data_4, alpha=0.5)\n",
    "    states_4 = list({ tag for sentence in train_data_4 for _, tag in sentence })\n",
    "    \n",
    "    overall_acc_4 = evaluate(test_data_4, states_4, init_probs_4, trans_probs_4, emission_probs_4, vocab_4, default_tag_4)\n",
    "    tag_acc_4 = per_tag_accuracy(test_data_4, states_4, init_probs_4, trans_probs_4, emission_probs_4, vocab_4, default_tag_4)\n",
    "    \n",
    "    print(\"\\n--- 4-Tag Model (Collapsed) Evaluation ---\")\n",
    "    print(\"Overall Accuracy:\", overall_acc_4)\n",
    "    print(\"Per-Tag Accuracy:\")\n",
    "    for tag, acc in sorted(tag_acc_4.items()):\n",
    "        print(f\"  {tag}: {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HMMTagger:\n",
    "    def __init__(self, smoothing=1e-8):\n",
    "        # Transition counts: counts for (prev_tag -> curr_tag)\n",
    "        self.transition_counts = defaultdict(Counter)\n",
    "        # Emission counts: counts for (prev_tag, curr_tag, word)\n",
    "        self.emission_counts = defaultdict(lambda: defaultdict(Counter))\n",
    "        self.transition_probs = defaultdict(dict)\n",
    "        self.emission_probs = defaultdict(lambda: defaultdict(dict))\n",
    "        self.tags = set()\n",
    "        self.vocab = set()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def train(self, tagged_sentences):\n",
    "        \"\"\"\n",
    "        Train on a list of sentences, each a list of (word, tag) tuples.\n",
    "        Emission probability is modeled as P(word | previous_tag, current_tag).\n",
    "        \"\"\"\n",
    "        for sentence in tagged_sentences:\n",
    "            prev_tag = \"<s>\"  # start-of-sentence symbol\n",
    "            for word, tag in sentence:\n",
    "                self.transition_counts[prev_tag][tag] += 1\n",
    "                self.emission_counts[prev_tag][tag][word] += 1\n",
    "                self.tags.add(tag)\n",
    "                self.vocab.add(word)\n",
    "                prev_tag = tag\n",
    "            # Transition from the final tag to the end-of-sentence marker\n",
    "            self.transition_counts[prev_tag][\"</s>\"] += 1\n",
    "\n",
    "        # Compute transition probabilities P(Tᵢ | Tᵢ₋₁)\n",
    "        for prev_tag, counter in self.transition_counts.items():\n",
    "            total = sum(counter.values())\n",
    "            for curr_tag, count in counter.items():\n",
    "                self.transition_probs[prev_tag][curr_tag] = count / total\n",
    "\n",
    "        # Compute emission probabilities P(word | previous_tag, current_tag)\n",
    "        for prev_tag, tag_dict in self.emission_counts.items():\n",
    "            for curr_tag, word_counter in tag_dict.items():\n",
    "                total = sum(word_counter.values())\n",
    "                for word, count in word_counter.items():\n",
    "                    self.emission_probs[prev_tag][curr_tag][word] = count / total\n",
    "\n",
    "    def viterbi(self, sentence):\n",
    "        \"\"\"\n",
    "        Uses the Viterbi algorithm (with log probabilities) to find the best tag sequence for a sentence.\n",
    "        \"\"\"\n",
    "        V = [{}]\n",
    "        backpointer = [{}]\n",
    "\n",
    "        # Initialization (assume start tag is <s>)\n",
    "        for tag in self.tags:\n",
    "            trans_prob = self.transition_probs.get(\"<s>\", {}).get(tag, self.smoothing)\n",
    "            emis_prob = self.emission_probs.get(\"<s>\", {}).get(tag, {}).get(sentence[0], self.smoothing)\n",
    "            V[0][tag] = math.log(trans_prob) + math.log(emis_prob)\n",
    "            backpointer[0][tag] = \"<s>\"\n",
    "\n",
    "        # Recursion\n",
    "        for t in range(1, len(sentence)):\n",
    "            V.append({})\n",
    "            backpointer.append({})\n",
    "            for curr_tag in self.tags:\n",
    "                best_score = float(\"-inf\")\n",
    "                best_prev = None\n",
    "                for prev_tag in self.tags:\n",
    "                    prev_score = V[t-1][prev_tag]\n",
    "                    trans_prob = self.transition_probs.get(prev_tag, {}).get(curr_tag, self.smoothing)\n",
    "                    emis_prob = self.emission_probs.get(prev_tag, {}).get(curr_tag, {}).get(sentence[t], self.smoothing)\n",
    "                    score = prev_score + math.log(trans_prob) + math.log(emis_prob)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_prev = prev_tag\n",
    "                V[t][curr_tag] = best_score\n",
    "                backpointer[t][curr_tag] = best_prev\n",
    "\n",
    "        # Termination (incorporate transition to end-of-sentence)\n",
    "        best_score = float(\"-inf\")\n",
    "        best_final_tag = None\n",
    "        t = len(sentence) - 1\n",
    "        for tag in self.tags:\n",
    "            term_prob = self.transition_probs.get(tag, {}).get(\"</s>\", self.smoothing)\n",
    "            score = V[t][tag] + math.log(term_prob)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_final_tag = tag\n",
    "\n",
    "        # Backtrace the best tag path\n",
    "        best_path = [best_final_tag]\n",
    "        for t in range(len(sentence) - 1, 0, -1):\n",
    "            best_path.insert(0, backpointer[t][best_path[0]])\n",
    "        return best_path\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    Load data from a JSON file.\n",
    "    Each entry in the file should be a list: [sentence string, list of POS tags].\n",
    "    The sentence is tokenized using whitespace.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    training_data = []\n",
    "    for entry in data:\n",
    "        sentence_str, tags = entry\n",
    "        words = sentence_str.split()\n",
    "        if len(words) != len(tags):\n",
    "            print(\"Warning: Token-tag count mismatch in sentence:\", sentence_str)\n",
    "        training_data.append(list(zip(words, tags)))\n",
    "    return training_data\n",
    "\n",
    "def reduce_tags_data(tagged_sentences, mapping):\n",
    "    \"\"\"\n",
    "    Convert each (word, tag) tuple in the data to use a reduced tag set.\n",
    "    mapping: dictionary mapping original tags to the reduced set.\n",
    "    \"\"\"\n",
    "    reduced_sentences = []\n",
    "    for sentence in tagged_sentences:\n",
    "        new_sentence = []\n",
    "        for word, tag in sentence:\n",
    "            new_tag = mapping.get(tag, tag)\n",
    "            new_sentence.append((word, new_tag))\n",
    "        reduced_sentences.append(new_sentence)\n",
    "    return reduced_sentences\n",
    "\n",
    "def evaluate_model_detailed(model, tagged_sentences):\n",
    "    \"\"\"\n",
    "    Computes overall token accuracy and per-tag accuracies.\n",
    "    Returns overall_accuracy and a dictionary mapping each tag to its accuracy.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    tag_total = defaultdict(int)\n",
    "    tag_correct = defaultdict(int)\n",
    "    \n",
    "    for sentence in tagged_sentences:\n",
    "        words = [word for word, tag in sentence]\n",
    "        true_tags = [tag for word, tag in sentence]\n",
    "        predicted_tags = model.viterbi(words)\n",
    "        for pred, true in zip(predicted_tags, true_tags):\n",
    "            total += 1\n",
    "            tag_total[true] += 1\n",
    "            if pred == true:\n",
    "                correct += 1\n",
    "                tag_correct[true] += 1\n",
    "                \n",
    "    overall_accuracy = correct / total if total > 0 else 0\n",
    "    per_tag_accuracy = {tag: (tag_correct[tag] / tag_total[tag] if tag_total[tag] > 0 else 0)\n",
    "                        for tag in tag_total}\n",
    "    return overall_accuracy, per_tag_accuracy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the PTB data from JSON file\n",
    "    data = load_data(\"penn-data.json\")\n",
    "    \n",
    "    # --------------------------\n",
    "    # 36-Tag Model\n",
    "    # --------------------------\n",
    "    hmm_36 = HMMTagger()\n",
    "    hmm_36.train(data)\n",
    "    overall_acc_36, per_tag_acc_36 = evaluate_model_detailed(hmm_36, data)\n",
    "    \n",
    "    # --------------------------\n",
    "    # 4-Tag Reduced Model\n",
    "    # --------------------------\n",
    "    # Mapping from 36 tags to 4 coarse tags: N (noun), V (verb), ADJ (adjective), ADV (adverb)\n",
    "    mapping = {\n",
    "        # Nouns\n",
    "        \"NN\": \"N\", \"NNS\": \"N\", \"NNP\": \"N\", \"NNPS\": \"N\",\n",
    "        # Verbs\n",
    "        \"VB\": \"V\", \"VBD\": \"V\", \"VBG\": \"V\", \"VBN\": \"V\", \"VBP\": \"V\", \"VBZ\": \"V\",\n",
    "        # Adjectives\n",
    "        \"JJ\": \"ADJ\", \"JJR\": \"ADJ\", \"JJS\": \"ADJ\",\n",
    "        # Adverbs\n",
    "        \"RB\": \"ADV\", \"RBR\": \"ADV\", \"RBS\": \"ADV\"\n",
    "    }\n",
    "    \n",
    "    data_4 = preprocess_4tag(data)\n",
    "    hmm_4 = HMMTagger()\n",
    "    hmm_4.train(data_4)\n",
    "    overall_acc_4, per_tag_acc_4 = evaluate_model_detailed(hmm_4, data_4)\n",
    "    \n",
    "    # The computed accuracies are now stored in:\n",
    "    # overall_acc_36, per_tag_acc_36: for the 36-tag model\n",
    "    # overall_acc_4, per_tag_acc_4: for the 4-tag reduced model\n",
    "    # These can now be used for further processing or reporting.\n",
    "    \n",
    "    # For example, you can access:\n",
    "    # overall_acc_36 -> overall accuracy for the 36-tag model\n",
    "    # per_tag_acc_36 -> dictionary of accuracies for each tag (36-tag model)\n",
    "    # overall_acc_4 -> overall accuracy for the 4-tag model\n",
    "    # per_tag_acc_4 -> dictionary of accuracies for each tag (4-tag model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class HMMTagger:\n",
    "    def __init__(self, smoothing=1e-8):\n",
    "        # Transition counts: counts for (prev_tag -> curr_tag)\n",
    "        self.transition_counts = defaultdict(Counter)\n",
    "        # Emission counts: counts for (prev_tag, curr_tag, word)\n",
    "        self.emission_counts = defaultdict(lambda: defaultdict(Counter))\n",
    "        self.transition_probs = defaultdict(dict)\n",
    "        self.emission_probs = defaultdict(lambda: defaultdict(dict))\n",
    "        self.tags = set()\n",
    "        self.vocab = set()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def train(self, tagged_sentences):\n",
    "        \"\"\"\n",
    "        Train on a list of sentences, each a list of (word, tag) tuples.\n",
    "        Emission probability is modeled as P(word | previous_tag, current_tag).\n",
    "        \"\"\"\n",
    "        for sentence in tagged_sentences:\n",
    "            prev_tag = \"<s>\"  # start-of-sentence symbol\n",
    "            for word, tag in sentence:\n",
    "                self.transition_counts[prev_tag][tag] += 1\n",
    "                self.emission_counts[prev_tag][tag][word] += 1\n",
    "                self.tags.add(tag)\n",
    "                self.vocab.add(word)\n",
    "                prev_tag = tag\n",
    "            # Transition from final tag to end-of-sentence marker\n",
    "            self.transition_counts[prev_tag][\"</s>\"] += 1\n",
    "\n",
    "        # Normalize transition probabilities P(T_i | T_(i-1))\n",
    "        for prev_tag, counter in self.transition_counts.items():\n",
    "            total = sum(counter.values())\n",
    "            for curr_tag, count in counter.items():\n",
    "                self.transition_probs[prev_tag][curr_tag] = count / total\n",
    "\n",
    "        # Normalize emission probabilities P(word | previous_tag, current_tag)\n",
    "        for prev_tag, tag_dict in self.emission_counts.items():\n",
    "            for curr_tag, word_counter in tag_dict.items():\n",
    "                total = sum(word_counter.values())\n",
    "                for word, count in word_counter.items():\n",
    "                    self.emission_probs[prev_tag][curr_tag][word] = count / total\n",
    "\n",
    "    def viterbi(self, sentence):\n",
    "        \"\"\"\n",
    "        Uses the Viterbi algorithm (with log probabilities) to find the best tag sequence for a sentence.\n",
    "        \"\"\"\n",
    "        V = [{}]\n",
    "        backpointer = [{}]\n",
    "\n",
    "        # Initialization (assume start tag is <s>)\n",
    "        for tag in self.tags:\n",
    "            trans_prob = self.transition_probs.get(\"<s>\", {}).get(tag, self.smoothing)\n",
    "            emis_prob = self.emission_probs.get(\"<s>\", {}).get(tag, {}).get(sentence[0], self.smoothing)\n",
    "            V[0][tag] = math.log(trans_prob) + math.log(emis_prob)\n",
    "            backpointer[0][tag] = \"<s>\"\n",
    "\n",
    "        # Recursion\n",
    "        for t in range(1, len(sentence)):\n",
    "            V.append({})\n",
    "            backpointer.append({})\n",
    "            for curr_tag in self.tags:\n",
    "                best_score = float(\"-inf\")\n",
    "                best_prev = None\n",
    "                for prev_tag in self.tags:\n",
    "                    prev_score = V[t-1][prev_tag]\n",
    "                    trans_prob = self.transition_probs.get(prev_tag, {}).get(curr_tag, self.smoothing)\n",
    "                    emis_prob = self.emission_probs.get(prev_tag, {}).get(curr_tag, {}).get(sentence[t], self.smoothing)\n",
    "                    score = prev_score + math.log(trans_prob) + math.log(emis_prob)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_prev = prev_tag\n",
    "                V[t][curr_tag] = best_score\n",
    "                backpointer[t][curr_tag] = best_prev\n",
    "\n",
    "        # Termination (incorporate transition to end-of-sentence)\n",
    "        best_score = float(\"-inf\")\n",
    "        best_final_tag = None\n",
    "        t = len(sentence) - 1\n",
    "        for tag in self.tags:\n",
    "            term_prob = self.transition_probs.get(tag, {}).get(\"</s>\", self.smoothing)\n",
    "            score = V[t][tag] + math.log(term_prob)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_final_tag = tag\n",
    "\n",
    "        # Backtrace the best tag path\n",
    "        best_path = [best_final_tag]\n",
    "        for t in range(len(sentence) - 1, 0, -1):\n",
    "            best_path.insert(0, backpointer[t][best_path[0]])\n",
    "        return best_path\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    Load data from a JSON file.\n",
    "    Each entry in the file is a two-element list:\n",
    "    [sentence string, list of POS tags].\n",
    "    The sentence is tokenized by splitting on whitespace.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    training_data = []\n",
    "    for entry in data:\n",
    "        sentence_str, tags = entry\n",
    "        words = sentence_str.split()\n",
    "        if len(words) != len(tags):\n",
    "            print(\"Warning: Token-tag count mismatch in sentence:\", sentence_str)\n",
    "        training_data.append(list(zip(words, tags)))\n",
    "    return training_data\n",
    "\n",
    "def reduce_tags_data(tagged_sentences, mapping):\n",
    "    \"\"\"\n",
    "    Convert each (word, tag) tuple in the data to use a reduced tag set.\n",
    "    mapping: dictionary mapping original tags to the reduced set.\n",
    "    \"\"\"\n",
    "    reduced_sentences = []\n",
    "    for sentence in tagged_sentences:\n",
    "        new_sentence = []\n",
    "        for word, tag in sentence:\n",
    "            new_tag = mapping.get(tag, tag)\n",
    "            new_sentence.append((word, new_tag))\n",
    "        reduced_sentences.append(new_sentence)\n",
    "    return reduced_sentences\n",
    "\n",
    "def evaluate_model_detailed(model, tagged_sentences):\n",
    "    \"\"\"\n",
    "    Computes overall token accuracy and per-tag accuracies.\n",
    "    Returns overall_accuracy and a dictionary mapping each tag to its accuracy.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    tag_total = defaultdict(int)\n",
    "    tag_correct = defaultdict(int)\n",
    "    \n",
    "    for sentence in tagged_sentences:\n",
    "        words = [word for word, tag in sentence]\n",
    "        true_tags = [tag for word, tag in sentence]\n",
    "        predicted_tags = model.viterbi(words)\n",
    "        for pred, true in zip(predicted_tags, true_tags):\n",
    "            total += 1\n",
    "            tag_total[true] += 1\n",
    "            if pred == true:\n",
    "                correct += 1\n",
    "                tag_correct[true] += 1\n",
    "                \n",
    "    overall_accuracy = correct / total if total > 0 else 0\n",
    "    per_tag_accuracy = {tag: (tag_correct[tag] / tag_total[tag] if tag_total[tag] > 0 else 0)\n",
    "                        for tag in tag_total}\n",
    "    return overall_accuracy, per_tag_accuracy\n",
    "\n",
    "def split_data(data, test_ratio=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Shuffle and split the data into training and test sets.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    random.shuffle(data)\n",
    "    split_point = int(len(data) * (1 - test_ratio))\n",
    "    return data[:split_point], data[split_point:]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the PTB data from JSON file\n",
    "    all_data = load_data(\"penn-data.json\")\n",
    "    \n",
    "    # Split data into training and test sets (e.g., 80% train, 20% test)\n",
    "    train_data, test_data = split_data(all_data, test_ratio=0.2)\n",
    "    \n",
    "    # --------------------------\n",
    "    # 36-Tag Model\n",
    "    # --------------------------\n",
    "    hmm_36 = HMMTagger()\n",
    "    hmm_36.train(train_data)\n",
    "    overall_acc_36, per_tag_acc_36 = evaluate_model_detailed(hmm_36, test_data)\n",
    "    \n",
    "    print(\"36-Tag Model Results (Test Set):\")\n",
    "    print(\"Overall Accuracy: {:.2f}%\".format(overall_acc_36 * 100))\n",
    "    print(\"Per-Tag Accuracies:\")\n",
    "    for tag, acc in per_tag_acc_36.items():\n",
    "        print(\"  {}: {:.2f}%\".format(tag, acc * 100))\n",
    "    \n",
    "    # --------------------------\n",
    "    # 4-Tag Reduced Model\n",
    "    # --------------------------\n",
    "    # Mapping from the full 36 tags to 4 coarse tags: N (noun), V (verb), ADJ (adjective), ADV (adverb)\n",
    "    mapping = {\n",
    "        # Nouns\n",
    "        \"NN\": \"N\", \"NNS\": \"N\", \"NNP\": \"N\", \"NNPS\": \"N\",\n",
    "        # Verbs\n",
    "        \"VB\": \"V\", \"VBD\": \"V\", \"VBG\": \"V\", \"VBN\": \"V\", \"VBP\": \"V\", \"VBZ\": \"V\",\n",
    "        # Adjectives\n",
    "        \"JJ\": \"ADJ\", \"JJR\": \"ADJ\", \"JJS\": \"ADJ\",\n",
    "        # Adverbs\n",
    "        \"RB\": \"ADV\", \"RBR\": \"ADV\", \"RBS\": \"ADV\"\n",
    "    }\n",
    "    \n",
    "    # Reduce the training and test data for the 4-tag model\n",
    "    train_data_4 = preprocess_4tag(train_data)\n",
    "    test_data_4 = preprocess_4tag(test_data)\n",
    "    \n",
    "    hmm_4 = HMMTagger()\n",
    "    hmm_4.train(train_data_4)\n",
    "    overall_acc_4, per_tag_acc_4 = evaluate_model_detailed(hmm_4, test_data_4)\n",
    "    \n",
    "    print(\"\\n4-Tag Reduced Model Results (Test Set):\")\n",
    "    print(\"Overall Accuracy: {:.2f}%\".format(overall_acc_4 * 100))\n",
    "    print(\"Per-Tag Accuracies:\")\n",
    "    for tag, acc in per_tag_acc_4.items():\n",
    "        print(\"  {}: {:.2f}%\".format(tag, acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
