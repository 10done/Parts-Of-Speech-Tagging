{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    sentences = []\n",
    "    for sentence, tags in data:\n",
    "        tokens = sentence.split()\n",
    "        sentence_with_tags = list(zip(tokens, tags))\n",
    "        sentences.append(sentence_with_tags)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(sentences, test_ratio=0.2):\n",
    "    random.shuffle(sentences)\n",
    "    split_idx = int(len(sentences) * (1 - test_ratio))\n",
    "    return sentences[:split_idx], sentences[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_4tags(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'N'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'V'\n",
    "    elif tag.startswith('JJ') or tag.startswith('RB'):\n",
    "        return 'A'\n",
    "    else:\n",
    "        return 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_4tag(sentences):\n",
    "    return [[(word, map_to_4tags(tag)) for (word, tag) in sentence] for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_b(train_sentences):\n",
    "    \"\"\"Train Second-Order HMM with proper trigram handling.\"\"\"\n",
    "    trigram_transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "    initial_bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "    initial_unigram_counts = defaultdict(int)\n",
    "    emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "    tag_counts = defaultdict(int)\n",
    "    all_tags = set()\n",
    "    all_words = set()\n",
    "\n",
    "    for sentence in train_sentences:\n",
    "        prev_prev_tag, prev_tag = None, None\n",
    "        for i, (word, tag) in enumerate(sentence):\n",
    "            all_tags.add(tag)\n",
    "            all_words.add(word)\n",
    "            tag_counts[tag] += 1\n",
    "            emission_counts[tag][word] += 1\n",
    "\n",
    "            if i == 0:\n",
    "                initial_unigram_counts[tag] += 1\n",
    "            elif i == 1:\n",
    "                initial_bigram_counts[prev_tag][tag] += 1\n",
    "            elif i >= 2:\n",
    "                trigram_transition_counts[(prev_prev_tag, prev_tag)][tag] += 1\n",
    "\n",
    "            prev_prev_tag, prev_tag = prev_tag, tag\n",
    "\n",
    "    # Smoothing and probabilities\n",
    "    num_tags = len(all_tags)\n",
    "    trigram_transition_probs = defaultdict(lambda: defaultdict(lambda: -math.inf))\n",
    "    for (u, v) in trigram_transition_counts:\n",
    "        total = sum(trigram_transition_counts[(u, v)].values()) + num_tags\n",
    "        for w in all_tags:\n",
    "            count = trigram_transition_counts[(u, v)].get(w, 0) + 1\n",
    "            trigram_transition_probs[(u, v)][w] = math.log(count / total)\n",
    "\n",
    "    initial_bigram_probs = defaultdict(lambda: defaultdict(lambda: -math.inf))\n",
    "    for u in initial_bigram_counts:\n",
    "        total = sum(initial_bigram_counts[u].values()) + num_tags\n",
    "        for v in all_tags:\n",
    "            count = initial_bigram_counts[u].get(v, 0) + 1\n",
    "            initial_bigram_probs[u][v] = math.log(count / total)\n",
    "\n",
    "    initial_unigram_probs = defaultdict(lambda: -math.inf)\n",
    "    total_initial = sum(initial_unigram_counts.values()) + num_tags\n",
    "    for tag in all_tags:\n",
    "        count = initial_unigram_counts.get(tag, 0) + 1\n",
    "        initial_unigram_probs[tag] = math.log(count / total_initial)\n",
    "\n",
    "    emission_probs = defaultdict(lambda: defaultdict(lambda: -math.inf))\n",
    "    femission_probs = defaultdict(lambda: defaultdict(lambda: -math.inf))\n",
    "    for tag in all_tags:\n",
    "        total = tag_counts[tag] + len(all_words) + 1\n",
    "        for word in all_words:\n",
    "          count = emission_counts[tag].get(word, 0)\n",
    "          emission_probs[tag][word] = math.log(count + 1) - math.log(total)\n",
    "        emission_probs[tag]['UNK'] = math.log(1) - math.log(total)\n",
    "\n",
    "    most_freq_tag = max(tag_counts, key=lambda k: tag_counts[k])\n",
    "    return (\n",
    "        initial_unigram_probs,\n",
    "        initial_bigram_probs,\n",
    "        trigram_transition_probs,\n",
    "        emission_probs,\n",
    "        most_freq_tag,\n",
    "        all_tags,\n",
    "        all_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_b(sentence, initial_uni, initial_bi, trigram_trans, emit_probs, mft, all_tags, all_words):\n",
    "    \"\"\"Fixed Viterbi for Second-Order HMM.\"\"\"\n",
    "    words = [word for word, _ in sentence]\n",
    "    n = len(words)\n",
    "    if n == 0:\n",
    "        return []\n",
    "\n",
    "    # DP tables\n",
    "    viterbi = [defaultdict(lambda: defaultdict(lambda: -math.inf))]\n",
    "    backpointers = [defaultdict(lambda: defaultdict(lambda: None))]\n",
    "\n",
    "    # Initialize first word\n",
    "    word = words[0]\n",
    "    for u in all_tags:\n",
    "        emit = emit_probs[u].get(word, emit_probs[u]['UNK'])\n",
    "        viterbi[0][u][u] = initial_uni[u] + emit\n",
    "\n",
    "    # Initialize second word\n",
    "    if n >= 2:\n",
    "        viterbi.append(defaultdict(lambda: defaultdict(lambda: -math.inf)))\n",
    "        backpointers.append(defaultdict(lambda: defaultdict(lambda: None)))\n",
    "        word = words[1]\n",
    "        for v in all_tags:\n",
    "            emit = emit_probs[v].get(word, emit_probs[v]['UNK'])\n",
    "            for u in all_tags:\n",
    "                prob = viterbi[0][u][u] + initial_bi[u][v] + emit\n",
    "                if prob > viterbi[1][u][v]:\n",
    "                    viterbi[1][u][v] = prob\n",
    "                    backpointers[1][u][v] = u\n",
    "\n",
    "    # Recursion for remaining words\n",
    "    for t in range(2, n):\n",
    "        viterbi.append(defaultdict(lambda: defaultdict(lambda: -math.inf)))\n",
    "        backpointers.append(defaultdict(lambda: defaultdict(lambda: None)))\n",
    "        word = words[t]\n",
    "        for w in all_tags:\n",
    "            emit = emit_probs[w].get(word, emit_probs[w]['UNK'])\n",
    "            for u in all_tags:\n",
    "                for v in all_tags:\n",
    "                    trigram_prob = trigram_trans[(u, v)].get(w, -math.inf)\n",
    "                    prev_prob = viterbi[t-1][u][v]\n",
    "                    total_prob = prev_prob + trigram_prob + emit\n",
    "                    if total_prob > viterbi[t][v][w]:\n",
    "                        viterbi[t][v][w] = total_prob\n",
    "                        backpointers[t][v][w] = (u, v)\n",
    "\n",
    "    # Backtracking\n",
    "    best_path = []\n",
    "    if n == 1:\n",
    "        best_u = max(all_tags, key=lambda u: viterbi[0][u][u])\n",
    "        best_path = [best_u]\n",
    "    else:\n",
    "        max_prob = -math.inf\n",
    "        best_u, best_v = None, None\n",
    "        for u in all_tags:\n",
    "            for v in all_tags:\n",
    "                if viterbi[-1][u][v] > max_prob:\n",
    "                    max_prob = viterbi[-1][u][v]\n",
    "                    best_u, best_v = u, v\n",
    "        best_path = [best_u, best_v]\n",
    "        for t in range(n-3, -1, -1):\n",
    "            prev_u, prev_v = backpointers[t+2][best_u][best_v]\n",
    "            best_path.insert(0, prev_u)\n",
    "            best_u, best_v = prev_v, best_u\n",
    "\n",
    "    return best_path[:n]  # Trim to sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_sentences, pred_sentences):\n",
    "    \"\"\"Compute overall and tag-wise accuracy.\"\"\"\n",
    "    total = correct = 0\n",
    "    tag_correct = defaultdict(int)\n",
    "    tag_total = defaultdict(int)\n",
    "    for true, pred in zip(true_sentences, pred_sentences):\n",
    "        for (_, true_tag), pred_tag in zip(true, pred):\n",
    "            total += 1\n",
    "            tag_total[true_tag] += 1\n",
    "            if true_tag == pred_tag:\n",
    "                correct += 1\n",
    "                tag_correct[true_tag] += 1\n",
    "    overall_acc = correct / total if total else 0\n",
    "    tag_acc = {tag: tag_correct[tag]/tag_total[tag] for tag in tag_total}\n",
    "    return overall_acc, tag_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 36-Tag (Second-Order HMM) ===\n",
      "Overall Accuracy: 0.6734\n",
      "Tag-wise Accuracy:\n",
      "#: 0.0000\n",
      "-LRB-: 0.5217\n",
      "-RRB-: 0.3478\n",
      ":: 0.4545\n",
      "CC: 0.9778\n",
      "CD: 0.4711\n",
      "DT: 0.9518\n",
      "EX: 0.1765\n",
      "FW: 0.0000\n",
      "IN: 0.9607\n",
      "JJ: 0.4941\n",
      "JJR: 0.1644\n",
      "JJS: 0.2286\n",
      "MD: 0.3769\n",
      "NN: 0.7094\n",
      "NNP: 0.6603\n",
      "NNPS: 0.0000\n",
      "NNS: 0.3669\n",
      "PDT: 0.0000\n",
      "PRP: 0.5510\n",
      "PRP$: 0.5432\n",
      "RB: 0.3802\n",
      "RBR: 0.0968\n",
      "RBS: 0.0000\n",
      "RP: 0.0909\n",
      "TO: 0.9429\n",
      "VB: 0.8345\n",
      "VBD: 0.7381\n",
      "VBG: 0.1672\n",
      "VBN: 0.3848\n",
      "VBP: 0.5885\n",
      "VBZ: 0.7494\n",
      "WDT: 0.1845\n",
      "WP: 0.2609\n",
      "WP$: 0.0000\n",
      "WRB: 0.3077\n",
      "\n",
      "=== 4-Tag (Second-Order HMM) ===\n",
      "Overall Accuracy: 0.8592\n",
      "Tag-wise Accuracy:\n",
      "N: 0.8546\n",
      "V: 0.7458\n",
      "A: 0.6608\n",
      "O: 0.9654\n"
     ]
    }
   ],
   "source": [
    "json_file = 'penn-data.json'  # Replace with your JSON file path\n",
    "sentences = load_data(json_file)\n",
    "train_36, test_36 = split_data(sentences)\n",
    "train_4 = preprocess_4tag(train_36)\n",
    "test_4 = preprocess_4tag(test_36)\n",
    "\n",
    "# Train models\n",
    "(initial_uni_36, initial_bi_36, trans_36, emit_36, mft_36, tags_36, words_36) = train_model_b(train_36)\n",
    "(initial_uni_4, initial_bi_4, trans_4, emit_4, mft_4, tags_4, words_4) = train_model_b(train_4)\n",
    "\n",
    "# Predict and evaluate\n",
    "preds_36 = [viterbi_b(s, initial_uni_36, initial_bi_36, trans_36, emit_36, mft_36, tags_36, words_36) for s in test_36]\n",
    "preds_4 = [viterbi_b(s, initial_uni_4, initial_bi_4, trans_4, emit_4, mft_4, tags_4, words_4) for s in test_4]\n",
    "\n",
    "acc_36, tag_acc_36 = evaluate(test_36, preds_36)\n",
    "acc_4, tag_acc_4 = evaluate(test_4, preds_4)\n",
    "\n",
    "# Print results\n",
    "print(\"=== 36-Tag (Second-Order HMM) ===\")\n",
    "print(f\"Overall Accuracy: {acc_36:.4f}\")\n",
    "print(\"Tag-wise Accuracy:\")\n",
    "for tag in sorted(tag_acc_36.keys()):\n",
    "    print(f\"{tag}: {tag_acc_36[tag]:.4f}\")\n",
    "\n",
    "print(\"\\n=== 4-Tag (Second-Order HMM) ===\")\n",
    "print(f\"Overall Accuracy: {acc_4:.4f}\")\n",
    "print(\"Tag-wise Accuracy:\")\n",
    "for tag in ['N', 'V', 'A', 'O']:\n",
    "    print(f\"{tag}: {tag_acc_4.get(tag, 0.0):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
